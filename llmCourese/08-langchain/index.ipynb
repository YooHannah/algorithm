{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 💡 这节课会带给你\n",
    "\n",
    "1. 如何使用 LangChain：一套在大模型能力上封装的工具框架\n",
    "2. 如何用几行代码实现一个复杂的 AI 应用\n",
    "3. 面向大模型的流程开发的过程抽象\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 这节课怎么学\n",
    "\n",
    "代码能力要求：**中高**，AI/数学基础要求：**无**\n",
    "\n",
    "1. 有编程基础的同学\n",
    "   - 关注设计思路，实现细节\n",
    "2. 没有编程基础的同学\n",
    "   - 尽量理解 SDK 的概念和价值，尝试体会使用 SDK 前后的差别与意义\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写在前面\n",
    "\n",
    "- LangChain 也是一套面向大模型的开发框架（SDK）\n",
    "- LangChain 是 AGI 时代软件工程的一个探索和原型\n",
    "- 学习 LangChain 要关注接口变更\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>温馨提示：</b>\n",
    "<ol>\n",
    "<li>实验室平台已经内置了本课件依赖所有的包，相关下载包命令已经注释，如果在本地运行相关代码，则需要安装所需依赖包</li>\n",
    "<li>实验室平台不支持 gpt-4 模型，如果需要体验 gpt-4 效果，请参考AGI课堂手册： https://a.agiclass.ai </li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 的核心组件\n",
    "\n",
    "1. 模型 I/O 封装\n",
    "   - LLMs：大语言模型\n",
    "   - Chat Models：一般基于 LLMs，但按对话结构重新封装\n",
    "   - PromptTemple：提示词模板\n",
    "   - OutputParser：解析输出\n",
    "2. 数据连接封装\n",
    "   - Document Loaders：各种格式文件的加载器\n",
    "   - Document Transformers：对文档的常用操作，如：split, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Models：文本向量化表示，用于检索等操作（啥意思？别急，后面详细讲）\n",
    "   - Verctorstores: （面向检索的）向量的存储\n",
    "   - Retrievers: 向量的检索\n",
    "3. 记忆封装\n",
    "   - Memory：这里不是物理内存，从文本的角度，可以理解为“上文”、“历史记录”或者说“记忆力”的管理\n",
    "4. 架构封装\n",
    "   - Chain：实现一个功能或者一系列顺序功能组合\n",
    "   - Agent：根据用户输入，自动规划执行步骤，自动选择每步需要的工具，最终完成用户指定的功能\n",
    "     - Tools：调用外部功能的函数，例如：调 google 搜索、文件 I/O、Linux Shell 等等\n",
    "     - Toolkits：操作某软件的一组工具集，例如：操作 DB、操作 Gmail 等等\n",
    "5. Callbacks\n",
    "\n",
    "<img src=\"langchain.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档（以 Python 版为例）\n",
    "\n",
    "- 功能模块：https://python.langchain.com/docs/get_started/introduction\n",
    "- API 文档：https://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "- 三方组件集成：https://python.langchain.com/docs/integrations/platforms/\n",
    "- 官方应用案例：https://python.langchain.com/docs/use_cases\n",
    "- 调试部署等指导：https://python.langchain.com/docs/guides/debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、模型 I/O 封装\n",
    "\n",
    "把不同的模型，统一封装成一个接口，方便更换模型而不用重构代码。\n",
    "\n",
    "### 1.1 模型 API：LLM vs. ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.20\n",
    "# !pip install langchain-openai==0.1.6\n",
    "# !pip install langchain-community==0.0.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与本课无关，请忽略\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"\n",
    "\n",
    "OPENAI_API_KEY = \"sk-xxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI 模型封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个AI助手，可以回答你的问题和提供帮助。您有什么需要我帮忙的吗？\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()  # 默认是gpt-3.5-turbo\n",
    "response = llm.invoke(\"你是谁\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 多轮对话 Session 封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您是王卓然先生。\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # 等价于OpenAI接口中的assistant role\n",
    "    HumanMessage,  # 等价于OpenAI接口中的user role\n",
    "    SystemMessage  # 等价于OpenAI接口中的system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"),\n",
    "    HumanMessage(content=\"我是学员，我叫王卓然。\"),\n",
    "    AIMessage(content=\"欢迎！\"),\n",
    "    HumanMessage(content=\"我是谁\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>通过模型封装，实现不同模型的统一接口调用\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 换个国产模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install qianfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2024-07-02 09:49:25.573] oauth.py:228 [t:139814599567168]: trying to refresh access_token for ak `cuTPS7***`\n",
      "[INFO][2024-07-02 09:49:26.322] oauth.py:243 [t:139814599567168]: sucessfully refresh access_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好，我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。\n",
      "\n",
      "如果您有任何问题，请随时告诉我。\n"
     ]
    }
   ],
   "source": [
    "# 其它模型分装在 langchain_community 底包中\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "llm = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"你是谁\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型的输入与输出\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 1.2.1 Prompt 模板封装\n",
    "\n",
    "1. PromptTemplate 可以在模板中自定义变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] template='给我讲个关于{subject}的笑话'\n",
      "===Prompt===\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='小明'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，这里有一个关于小明的笑话：\n",
      "\n",
      "小明问爸爸：“爸爸，你知道为什么海水是咸的吗？”\n",
      "爸爸说：“为什么？”\n",
      "小明说：“因为鱼都在里面尿尿！”\n",
      "爸爸听完后笑得前仰后合，然后说：“小明，你这个笑话真是太咸了！”\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 定义 LLM\n",
    "llm = ChatOpenAI()\n",
    "# 通过 Prompt 调用 LLM\n",
    "ret = llm.invoke(template.format(subject='小明'))\n",
    "# 打印输出\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate 用模板表示的对话上下文\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，我是AGI课堂的客服助手，名字叫瓜瓜。有什么可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"你是{product}的客服助手。你的名字叫{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "    product=\"AGI课堂\",\n",
    "    name=\"瓜瓜\",\n",
    "    query=\"你是谁\"\n",
    ")\n",
    "\n",
    "ret = llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder 把多轮对话变成模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name 是 message placeholder 在模板中的变量名\n",
    "    # 用于在赋值时使用\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?'), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer'), HumanMessage(content='Translate your answer to 中文.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # 对 \"conversation\" 和 \"language\" 赋值\n",
    "    conversation=[human_message, ai_message], language=\"中文\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "埃隆·马斯克（Elon Musk）是一位亿万富翁企业家、发明家和工业设计师。\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>把Prompt模板看作带有参数的函数\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2、从文件加载 Prompt 模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] template='举一个关于{topic}的例子'\n",
      "===Prompt===\n",
      "举一个关于黑色幽默的例子\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='黑色幽默'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 输出封装 OutputParser\n",
    "\n",
    "自动把 LLM 输出的字符串按指定格式加载。\n",
    "\n",
    "LangChain 内置的 OutputParser 包括:\n",
    "\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- JsonOutputParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "\n",
    "等等\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "\n",
    "自动根据 Pydantic 类的定义，生成输出的格式说明\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "\n",
    "# 定义你的输出对象\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "提取用户输入中的日期。\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "用户输入:\n",
      "2023年四月6日天气晴...\n",
      "====模型原始输出=====\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 4,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "====Parse后的输出=====\n",
      "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# 根据Pydantic对象的定义，构造一个OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"提取用户输入中的日期。\n",
    "{format_instructions}\n",
    "用户输入:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # 直接从OutputParser中获取输出描述，并对模板的变量预先赋值\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "query = \"2023年四月6日天气晴...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input.to_string())\n",
    "\n",
    "output = model.invoke(model_input.to_messages())\n",
    "print(\"====模型原始输出=====\")\n",
    "print(output.content)\n",
    "print(\"====Parse后的输出=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "利用 LLM 自动根据解析异常修复并重新解析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===格式错误的Output===\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四月,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===出现异常===\n",
      "Invalid json output: {\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四月,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===重新解析结果===\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(\n",
    "    parser=parser, llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# 我们把之前output的格式改错\n",
    "output = output.content.replace(\"4\", \"四月\")\n",
    "print(\"===格式错误的Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===出现异常===\")\n",
    "    print(e)\n",
    "\n",
    "# 用OutputFixingParser自动修复并解析\n",
    "date = new_parser.parse(output)\n",
    "print(\"===重新解析结果===\")\n",
    "print(date.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>猜一下OutputFixingParser是怎么做到的\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、小结\n",
    "\n",
    "1. LangChain 统一封装了各种模型的调用接口，包括补全型和对话型两种\n",
    "2. LangChain 提供了 PromptTemplate 类，可以自定义带变量的模板\n",
    "3. LangChain 提供了一些列输出解析器，用于将大模型的输出解析成结构化对象；额外带有自动修复功能。\n",
    "4. 上述模型属于 LangChain 中较为优秀的部分；美中不足的是 OutputParser 自身的 Prompt 维护在代码中，耦合度较高。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据连接封装\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 文档加载器：Document Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 文档处理器\n",
    "\n",
    "### 2.2.1 TextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "-------\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "-------\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "-------\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "-------\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "-------\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "-------\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "-------\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "-------\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "-------\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "-------\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "-------\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100, \n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "类似 LlamaIndex，LangChain 也提供了丰富的 <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#document-loaders\">Document Loaders</a></code> 和 <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#text-splitters\">Text Splitters</a></code>。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3、向量数据库与向量检索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but are not releasing.§\n",
      "2. Llama 2-Chat, a ﬁne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs,\n",
      "----\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "----\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 加载文档\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# 灌库\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 检索 top-5 结果\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "docs = retriever.invoke(\"llama2有多少参数\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的三方检索组件链接，参考：https://python.langchain.com/docs/integrations/vectorstores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4、小结\n",
    "\n",
    "1. 文档处理部分，建议在实际应用中详细测试后使用\n",
    "2. 与向量数据库的链接部分本质是接口封装，向量数据库需要自己选型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、记忆封装：Memory\n",
    "\n",
    "### 3.1、对话上下文：ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好啊\\nAI: 你也好啊'}\n",
      "{'history': 'Human: 你好啊\\nAI: 你也好啊\\nHuman: 你再好啊\\nAI: 你又好啊'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"你好啊\"}, {\"output\": \"你也好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"你再好啊\"}, {\"output\": \"你又好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2、只保留一个窗口的上下文：ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 第二轮问\\nAI: 第二轮答\\nHuman: 第三轮问\\nAI: 第三轮答'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"第一轮问\"}, {\"output\": \"第一轮答\"})\n",
    "window.save_context({\"input\": \"第二轮问\"}, {\"output\": \"第二轮答\"})\n",
    "window.save_context({\"input\": \"第三轮问\"}, {\"output\": \"第三轮答\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3、通过 Token 数控制上下文长度：ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: 你好，我是你的AI助手。\\nHuman: 你会干什么\\nAI: 我什么都会'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=ChatOpenAI(),\n",
    "    max_token_limit=40\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"你好啊\"}, {\"output\": \"你好，我是你的AI助手。\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"你会干什么\"}, {\"output\": \"我什么都会\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4、更多类型\n",
    "\n",
    "- ConversationSummaryMemory: 对上下文做摘要\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary\n",
    "- ConversationSummaryBufferMemory: 保存 Token 数限制内的上下文，对更早的做摘要\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary_buffer\n",
    "- VectorStoreRetrieverMemory: 将 Memory 存储在向量数据库中，根据用户输入检索回最相关的部分\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5、小结\n",
    "\n",
    "1. LangChain 的 Memory 管理机制属于可用的部分，尤其是简单情况如按轮数或按 Token 数管理；\n",
    "2. 对于复杂情况，它不一定是最优的实现，例如检索向量库方式，建议根据实际情况和效果评估；\n",
    "3. 但是**它对内存的各种维护方法的思路在实际生产中可以借鉴**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、Chain 和 LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Language（LCEL）是一种声明式语言，可轻松组合不同的调用顺序构成 Chain。LCEL 自创立之初就被设计为能够支持将原型投入生产环境，**无需代码更改**，从最简单的“提示+LLM”链到最复杂的链（已有用户成功在生产环境中运行包含数百个步骤的 LCEL Chain）。\n",
    "\n",
    "LCEL 的一些亮点包括：\n",
    "\n",
    "1. **流支持**：使用 LCEL 构建 Chain 时，你可以获得最佳的首个令牌时间（即从输出开始到首批输出生成的时间）。对于某些 Chain，这意味着可以直接从 LLM 流式传输令牌到流输出解析器，从而以与 LLM 提供商输出原始令牌相同的速率获得解析后的、增量的输出。\n",
    "\n",
    "2. **异步支持**：任何使用 LCEL 构建的链条都可以通过同步 API（例如，在 Jupyter 笔记本中进行原型设计时）和异步 API（例如，在 LangServe 服务器中）调用。这使得相同的代码可用于原型设计和生产环境，具有出色的性能，并能够在同一服务器中处理多个并发请求。\n",
    "\n",
    "3. **优化的并行执行**：当你的 LCEL 链条有可以并行执行的步骤时（例如，从多个检索器中获取文档），我们会自动执行，无论是在同步还是异步接口中，以实现最小的延迟。\n",
    "\n",
    "4. **重试和回退**：为 LCEL 链的任何部分配置重试和回退。这是使链在规模上更可靠的绝佳方式。目前我们正在添加重试/回退的流媒体支持，因此你可以在不增加任何延迟成本的情况下获得增加的可靠性。\n",
    "\n",
    "5. **访问中间结果**：对于更复杂的链条，访问在最终输出产生之前的中间步骤的结果通常非常有用。这可以用于让最终用户知道正在发生一些事情，甚至仅用于调试链条。你可以流式传输中间结果，并且在每个 LangServe 服务器上都可用。\n",
    "\n",
    "6. **输入和输出模式**：输入和输出模式为每个 LCEL 链提供了从链的结构推断出的 Pydantic 和 JSONSchema 模式。这可以用于输入和输出的验证，是 LangServe 的一个组成部分。\n",
    "\n",
    "7. **无缝 LangSmith 跟踪集成**：随着链条变得越来越复杂，理解每一步发生了什么变得越来越重要。通过 LCEL，所有步骤都自动记录到 LangSmith，以实现最大的可观察性和可调试性。\n",
    "\n",
    "8. **无缝 LangServe 部署集成**：任何使用 LCEL 创建的链都可以轻松地使用 LangServe 进行部署。\n",
    "\n",
    "原文：https://python.langchain.com/docs/expression_language/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline 式调用 PromptTemplate, LLM 和 OutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 输出结构\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"流量包名称\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"价格下限\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"价格上限\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"流量下限\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"流量上限\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"按价格或流量排序\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(\n",
    "        description=\"升序或降序排列\", default=None)\n",
    "\n",
    "\n",
    "# OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Semantics)\n",
    "\n",
    "# Prompt 模板\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"将用户的输入解析成JSON表示。输出格式如下：\\n{format_instructions}\\n不要输出未提及的字段。\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# 模型\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# LCEL 表达式\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | model | parser\n",
    ")\n",
    "\n",
    "# 直接运行\n",
    "ret = runnable.invoke(\"不超过100元的流量大的套餐有哪些\")\n",
    "print(\n",
    "    json.dumps(\n",
    "        ret.dict(),\n",
    "        indent = 4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"price_upper\": 100,\n",
      "  \"sort_by\": \"data\",\n",
      "  \"ordering\": \"descend\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# 流式输出\n",
    "for s in runnable.stream(\"不超过100元的流量大的套餐有哪些\"):\n",
    "    print(s, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>注意:</b> 在当前的文档中 LCEL 产生的对象，被叫做 runnable 或 chain，经常两种叫法混用。本质就是一个自定义调用流程。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>使用 LCEL 的价值，也就是 LangChain 的核心价值。</b> <br />\n",
    "官方从不同角度给出了举例说明：<a href=\"https://python.langchain.com/docs/expression_language/why\">https://python.langchain.com/docs/expression_language/why</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 用 LCEL 实现 RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 加载文档\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# 文档切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# 灌库\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# 检索 top-1 结果\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2有7B, 13B, 和70B参数的变体。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Prompt模板\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Llama 2有多少参数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 通过 LCEL 实现 Function Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"两个整数相乘\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"\"\"Exponentiate the base to the exponent power.\"\"\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import JsonOutputToolsParser\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "# 带有分支的 LCEL\n",
    "llm_with_tools = model.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser(),\n",
    "    \"text\": StrOutputParser()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [{'args': {'first_int': 1024, 'second_int': 16}, 'type': 'multiply'}], 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"1024的16倍是多少\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [], 'text': '我是一个由OpenAI开发的人工智能助手，旨在帮助你解决各种问题。你可以问我问题，寻求建议，或者请求帮助完成任务。有什么我可以帮你的吗？'}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"你是谁\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接选择工具并运行（选）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "# 名称到函数的映射\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "def call_tool(tool_invocation: dict) -> Union[str, Runnable]:\n",
    "    \"\"\"根据模型选择的 tool 动态创建 LCEL\"\"\"\n",
    "    tool = tool_map[tool_invocation[\"type\"]]\n",
    "    return RunnablePassthrough.assign(\n",
    "        output=itemgetter(\"args\") | tool\n",
    "    )\n",
    "\n",
    "\n",
    "# .map() 使 function 逐一作用与一组输入\n",
    "call_tool_list = RunnableLambda(call_tool).map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': {'base': 1024, 'exponent': 2}, 'type': 'exponentiate', 'output': 1048576}]\n",
      "你好！有什么我可以帮忙的吗？\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def route(response):\n",
    "    if len(response[\"functions\"]) > 0:\n",
    "        return response[\"functions\"]\n",
    "    else:\n",
    "        return response[\"text\"]\n",
    "\n",
    "\n",
    "llm_with_tools = model.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser() | call_tool_list,\n",
    "    \"text\": StrOutputParser()\n",
    "} | RunnableLambda(route)\n",
    "\n",
    "result = llm_with_tools.invoke(\"1024的平方是多少\")\n",
    "print(result)\n",
    "\n",
    "result = llm_with_tools.invoke(\"你好\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "这种写法可读性太差了，个人不建议使用过于复杂的 LCEL！\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 用 LCEL 实现工厂模式（选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，我是一个基于人工智能技术的语言模型助手，目前由OpenAI公司开发和维护。我可以回答各种问题，提供信息和帮助解决问题。希望我可以帮到你！如果有任何问题或需要帮助，请随时告诉我。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# 模型1\n",
    "ernie_model = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "# 模型2\n",
    "gpt_model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# 通过 configurable_alternatives 按指定字段选择模型\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\", \n",
    "    ernie=ernie_model,\n",
    "    # claude=claude_model,\n",
    ")\n",
    "\n",
    "# Prompt 模板\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 运行时指定模型 \"gpt\" or \"ernie\"\n",
    "ret = chain.with_config(configurable={\"llm\": \"gpt\"}).invoke(\"介绍你自己，包括你的生产商\")\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扩展阅读：什么是[**工厂模式**](https://www.runoob.com/design-pattern/factory-pattern.html)；[**设计模式**](https://www.runoob.com/design-pattern/design-pattern-intro.html)概览。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>从模块间解依赖角度，LCEL的意义是什么？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过 LCEL，还可以实现\n",
    "\n",
    "1. 配置运行时变量：https://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. 故障回退：https://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. 并行调用：https://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. 逻辑分支：https://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. 调用自定义流式函数：https://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. 链接外部 Memory：https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "更多例子：https://python.langchain.com/docs/expression_language/cookbook/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、智能体架构：Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 回忆：什么是智能体（Agent）\n",
    "\n",
    "将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 先定义一些工具：Tools\n",
    "\n",
    "- 可以是一个函数或三方 API\n",
    "- 也可以把一个 Chain 或者 Agent 的 run()作为一个 Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注册 [SerpAPI](https://serpapi.com/)（限量免费），并将 `SERPAPI_API_KEY` 写在环境变量中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "# 自定义工具\n",
    "\n",
    "\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "\n",
    "tools += [weekday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 智能体类型：ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import json\n",
    "\n",
    "# 下载一个现有的 Prompt 模板\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe question is asking for the day of the week for Jay Chou's concert in 2024. To answer this, I need to know the specific date of the concert.\n",
      "\n",
      "Action: Search\n",
      "Action Input: Jay Chou concert date 2024\u001b[0m\u001b[36;1m\u001b[1;3mThe \"Carnival 2024\" World Tour will take place on October 11th, 12th, and 13th (three nights) at the Singapore National Stadium, with priority booking tickets starting at 10:00 AM on May 29th!\u001b[0m\u001b[32;1m\u001b[1;3mThe concert dates for Jay Chou's \"Carnival 2024\" World Tour are October 11th, 12th, and 13th, 2024. I need to determine the days of the week for these dates.\n",
      "\n",
      "Action: weekday\n",
      "Action Input: 2024-10-11\u001b[0m\u001b[33;1m\u001b[1;3mFriday\u001b[0m\u001b[32;1m\u001b[1;3mAction: weekday\n",
      "Action Input: 2024-10-12\u001b[0m\u001b[33;1m\u001b[1;3mSaturday\u001b[0m\u001b[32;1m\u001b[1;3mAction: weekday\n",
      "Action Input: 2024-10-13\u001b[0m\u001b[33;1m\u001b[1;3mSunday\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: Jay Chou's concerts in 2024 will be on Friday, October 11th, Saturday, October 12th, and Sunday, October 13th.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '2024年周杰伦的演唱会星期几',\n",
       " 'output': \"Jay Chou's concerts in 2024 will be on Friday, October 11th, Saturday, October 12th, and Sunday, October 13th.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0, model_kwargs={\"seed\":23})\n",
    "\n",
    "# 定义一个 agent: 需要大模型、工具集、和 Prompt 模板\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "# 定义一个执行器：需要 agent 对象 和 工具集\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# 执行\n",
    "agent_executor.invoke({\"input\": \"2024年周杰伦的演唱会星期几\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 智能体类型：SelfAskWithSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# 下载一个模板\n",
    "self_ask_prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "\n",
    "print(self_ask_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes.\n",
      "\n",
      "Follow up: 冯小刚的老婆是谁？\u001b[0m\u001b[36;1m\u001b[1;3m['Feng Xiaogang is a Chinese film director, screenwriter, actor, producer and politician. He is well known in China as a highly successful commercial filmmaker whose comedy films do consistently well at the box office, although Feng has broken out from that mold by making some drama and period drama films.', 'Feng Xiaogang (冯小刚) type: Chinese film director and screenwriter.', 'Feng Xiaogang (冯小刚) entity_type: people, people.', 'Feng Xiaogang (冯小刚) kgmid: /m/04xhrq.', 'Feng Xiaogang (冯小刚) born: 1958 (age 66 years), Daxing District, Beijing, China.', 'Feng Xiaogang (冯小刚) awards: Golden Horse Award for Best Leading Actor.', 'Feng Xiaogang (冯小刚) children: Siyu Feng.', 'Feng Xiaogang (冯小刚) tv_shows: Crossroad Bistro, Bian ji bu de gu shi, Hidden Energy, The Legendary Swordsman, Kai xuan zai zi ye.', 'Feng Xiaogang (冯小刚) production_designed: Father.', '冯小刚妻子徐帆，是内地知名女演员，文艺世家出身，1991年，24岁的她毕业于央戏表演专业。当时徐帆刚和王志文分手，很是失落，于是冯小刚时常安慰和开导她 ...', '冯小刚老婆徐帆和好友热聊，心情大好，面带笑容，还和朋友相拥送别。从图片看，身旁的好友是亚洲面孔。 ... 1999年，冯小刚和第一任妻子正式离婚，同年迎娶 ...', '徐帆是著名导演冯小刚的妻子，可以说是家喻户晓的实力派女星。值得一提的是，如今已然五十二岁的徐帆看上去依旧是靓丽又显年轻，整个人没有半分老态， ...', '不过这场婚姻并没有持续多久，随后前妻张娣为冯小刚生下一个女儿，但是冯小刚却在1993年的时候拍摄《大撒把》时又对徐帆心生爱意，两人偷偷相恋，后来在 ...', '最佳答案: 1. 冯小刚的现任妻子是徐帆，她是一位著名的中国女演员，出生于1967年8月8日，毕业于中央戏剧学院表演系。2. 徐帆的演艺生涯始于1987年，她在电视剧《青春无悔》 ...', '冯小刚（1958年3月18日—），籍贯湖南湘潭，出生于北京，中国大陆演员、导演、编剧。妻子是演员徐帆。冯小刚以北方京味儿喜剧著称，其作品开创了中国贺岁片市场。', '此外，那么爱冯小刚的徐帆，自1999年结婚至今24年都没为他生下一儿半女，反而选择领养一个小姑娘，这事儿多少也让人有点好奇。', '2任。冯小刚一共有2任老婆2段婚姻，原配妻子是张娣。冯小刚，1958年3月18日出生于北京市大兴区，祖籍湖南省湘潭市，中国内地导演、编剧、演员。第十三届全国政协文化文史和 ...', '现在很多人知道徐帆，是因为“她是冯小刚的老婆”。 但徐帆火的时候，也一样是国民女神级别的。 她出身文艺世家，父母都是楚剧演员，家境很好。偏偏徐帆 ...']\u001b[0m\u001b[32;1m\u001b[1;3mCould not parse output: Intermediate answer: 冯小刚的老婆是徐帆。\n",
      "\n",
      "Follow up: 徐帆演过哪些电影？\n",
      "\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mIntermediate answer: 徐帆演过的电影包括《唐山大地震》、《一九四二》、《手机》、《非诚勿扰》、《甲方乙方》等。\n",
      "\n",
      "So the final answer is: 徐帆演过的电影包括《唐山大地震》、《一九四二》、《手机》、《非诚勿扰》、《甲方乙方》等。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '冯小刚的老婆演过哪些电影，用中文回答',\n",
       " 'output': '徐帆演过的电影包括《唐山大地震》、《一九四二》、《手机》、《非诚勿扰》、《甲方乙方》等。'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"搜素引擎\",\n",
    "        max_results=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# self_ask_with_search_agent 只能传一个名为 'Intermediate Answer' 的 tool\n",
    "agent = create_self_ask_with_search_agent(llm, tools, self_ask_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"冯小刚的老婆演过哪些电影，用中文回答\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "Agent落地应用需要更多细节，后面课程中我们会专门讲 Agent 的实现\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、LangServe\n",
    "\n",
    "LangServe 用于将 Chain 或者 Runnable 部署成一个 REST API 服务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 LangServe\n",
    "# !pip install --upgrade \"langserve[all]\"\n",
    "\n",
    "# 也可以只安装一端\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1、Server 端\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"讲一个关于{topic}的笑话\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=9999)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2、Client 端\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:9999/joke/invoke\",\n",
    "    json={'input': {'topic': '小明'}}\n",
    ")\n",
    "print(response.json())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、LangChain.js\n",
    "\n",
    "Python 版 LangChain 的姊妹项目，都是由 Harrison Chase 主理。\n",
    "\n",
    "项目地址：https://github.com/langchain-ai/langchainjs\n",
    "\n",
    "文档地址：https://js.langchain.com/docs/\n",
    "\n",
    "特色：\n",
    "\n",
    "1. 可以和 Python 版 LangChain 无缝对接\n",
    "\n",
    "2. 抽象设计完全相同，概念一一对应\n",
    "\n",
    "3. 所有对象序列化后都能跨语言使用，但 API 差别挺大，不过在努力对齐\n",
    "\n",
    "支持环境：\n",
    "\n",
    "1. Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x\n",
    "2. Cloudflare Workers\n",
    "3. Vercel / Next.js (Browser, Serverless and Edge functions)\n",
    "4. Supabase Edge Functions\n",
    "5. Browser\n",
    "6. Deno\n",
    "\n",
    "安装：\n",
    "\n",
    "```\n",
    "npm install langchain\n",
    "```\n",
    "\n",
    "当前重点：\n",
    "\n",
    "1. 追上 Python 版的能力（甚至为此做了一个基于 gpt-3.5-turbo 的代码翻译器）\n",
    "2. 保持兼容尽可能多的环境\n",
    "3. 对质量关注不多，随时间自然能解决\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 与 LlamaIndex 的错位竞争\n",
    "\n",
    "- LangChain 侧重与 LLM 本身交互的封装\n",
    "  - Prompt、LLM、Memory、OutputParser 等工具丰富\n",
    "  - 在数据处理和 RAG 方面提供的工具相对粗糙\n",
    "  - 主打 LCEL 流程封装\n",
    "  - 配套 Agent、LangGraph 等智能体与工作流工具\n",
    "  - 另有 LangServe 部署工具和 LangSmith 监控调试工具\n",
    "- LlamaIndex 侧重与数据交互的封装\n",
    "  - 数据加载、切割、索引、检索、排序等相关工具丰富\n",
    "  - Prompt、LLM 等底层封装相对单薄\n",
    "  - 配套实现 RAG 相关工具\n",
    "  - 有 Agent 相关工具，不突出\n",
    "- LlamaIndex 为 LangChain 提供了集成\n",
    "  - 在 LlamaIndex 中调用 LangChain 封装的 LLM 接口：https://docs.llamaindex.ai/en/stable/api_reference/llms/langchain/\n",
    "  - 将 LlamaIndex 的 Query Engine 作为 LangChain Agent 的工具：https://docs.llamaindex.ai/en/v0.10.17/community/integrations/using_with_langchain.html\n",
    "  - LangChain 也 *曾经* 集成过 LlamaIndex，目前相关接口仍在：https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.llama_index.LlamaIndexRetriever.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "1. LangChain 随着版本迭代可用性有明显提升\n",
    "2. 使用 LangChain 要注意维护自己的 Prompt，尽量 Prompt 与代码逻辑解依赖\n",
    "3. 它的内置基础工具，建议充分测试效果后再决定是否使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 LangChain 重构 ChatPDF 的作业\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
